
model_type: dit
architectures: [
    DiT
]

hidden_size: 128
mlp_size: 256

num_layers: 6
num_attention_heads: 8

num_channels: 3
patch_size: 16

in_kernel_size: 5
out_kernel_size: 5

activation_fn: gelu_pytorch_tanh
use_qkv_bias: true
use_qk_norm: true

initializer_range: 0.02
norm_eps: 0.00001

gradient_checkpointing: False

scheduler: 
  beta_end: 0.012
  beta_schedule: scaled_linear
  beta_start: 0.00085
  num_train_timesteps: 1000
  set_alpha_to_one: false
  skip_prk_steps: true
  steps_offset: 1
  trained_betas: null
  clip_sample: false